apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: vllm-sample
  namespace: kserve
spec:
  predictor:
    model:
      modelFormat:
        name: vllm
      runtime: vllm-runtime
      storageUri: s3://my-models-dev/vllm/ # replace with your model path
